# üöÄ ETL Data Pipeline ‚Äî Python + PostgreSQL + Docker

This project implements a **modular ETL (Extract‚ÄìTransform‚ÄìLoad) pipeline** using:

* **Python 3.11+**
* **PostgreSQL 15** (Dockerized)
* **Docker Compose**
* **YAML-based configuration**
* **Structured logging**
* **Configurable ingestion (CSV, JSON, API)**

The pipeline reads raw files, validates and cleans data, loads it into PostgreSQL, and logs every step.

---

## üìÅ Project Structure

```
Revature-Cognizant-Project1/
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py               # ETL entrypoint
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db.py             # DB connection handling
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger.py         # Project-wide logging
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.py         # YAML configuration loader
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml           # DB + ETL settings
‚îÇ
‚îú‚îÄ‚îÄ data/                     # (mounted) input/output files
‚îÇ
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ etl.Dockerfile        # Dockerfile for ETL container
‚îÇ   ‚îú‚îÄ‚îÄ postgresql.conf       # (optional) custom DB config
‚îÇ   ‚îî‚îÄ‚îÄ init.sql              # (optional) bootstrap SQL
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env                      # DO NOT COMMIT (contains secrets)
‚îî‚îÄ‚îÄ README.md
```

---

## üîß Technologies Used

| Component  | Technology              |
| ---------- | ----------------------- |
| Language   | Python 3.11             |
| DB         | PostgreSQL 15 (Docker)  |
| UI         | pgAdmin 4 (Docker)      |
| Config     | YAML                    |
| Logging    | Python logging module   |
| Deployment | Docker & Docker Compose |

---

## üö® Environment Setup

### 1. Install dependencies (local development)

```bash
python -m venv .venv
.\.venv\Scripts\activate
pip install -r requirements.txt
```

---

### 2. Create `.env` (NOT committed to Git)

```
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=postgres
POSTGRES_PORT=5434
```

> üí° **We use port `5434` instead of `5432`** to avoid conflicts with any local PostgreSQL installation.

---

### 3. Start Postgres & pgAdmin via Docker

```bash
docker compose up -d postgres pgadmin
```

Check running containers:

```bash
docker compose ps
```

---

### 4. Connect with pgAdmin

* URL: [http://localhost:8080](http://localhost:8080)
* Email: `admin@example.com`
* Password: `admin`

Create new server:

| Field    | Value       |
| -------- | ----------- |
| Host     | `localhost` |
| Port     | `5434`      |
| Username | from `.env` |
| Password | from `.env` |
| Database | `postgres`  |

---

## ‚ñ∂Ô∏è Running the ETL Pipeline (2 ways)

### **Option A ‚Äî Run ETL locally (recommended during dev)**

```bash
python src/main.py
```

### **Option B ‚Äî Run ETL inside Docker**

```bash
docker compose up --build etl
```

Or for logs:

```bash
docker compose logs -f etl
```

---

## ‚öôÔ∏è Configuration (config/config.yaml)

Your ETL + DB settings live in:

```yaml
database:
  host: localhost
  port: 5434
  user: postgres
  password: postgres
  name: postgres

logging:
  log_dir: logs
  log_level: INFO
  log_format: "%(asctime)s | %(levelname)s | %(name)s | %(message)s"
```

You may add:

* input file paths
* ingestion schedules
* API endpoint settings
* validation rules

---

## üß™ Verifying DB Connection

Inside project root:

```bash
cd src
python -c "from utils.db import get_db_connection; c = get_db_connection(); print('connected:', c is not None); c and c.close()"
```

Expected:

```
connected: True
```

---

## üê≥ Docker Compose Services

### **etl**

* Runs Python ETL in container
* Mounted `/data` for real file access
* Uses `.env` for DB values

### **postgres**

* PostgreSQL 15 server
* Port mapped `5434:5432`
* Data persisted in Docker volume

### **pgadmin**

* Database GUI
* Accessible on `http://localhost:8080`

---

## üì¶ Deployment

To deploy everything in containers:

```bash
docker compose up --build -d
```

To stop:

```bash
docker compose down
```

---

## üîí Security Notes

* **Never commit `.env`**
* Commit **`.env.example`** instead:

```
POSTGRES_USER=
POSTGRES_PASSWORD=
POSTGRES_DB=
POSTGRES_PORT=
```

* Use environment variables in production (e.g., Docker Secrets)

---

## üìù Git Workflow

Since you started on `main` but want to push code to a branch:

```bash
git checkout -b feature/etl-setup
git push -u origin feature/etl-setup
```

---

## üìå Future Enhancements (optional)

* Add Alembic for DB migrations
* Add CSV/JSON ingestion pipelines
* Add API ingestion
* Add data validation layer (Pydantic)
* Add unit tests
* Add cron-based scheduling
* Add Airflow integration